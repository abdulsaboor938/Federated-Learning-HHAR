{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "from rbf_layer import RBFLayer, l_norm, rbf_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "M_PI=3.1416\n",
    "def compute_roll_yaw_pitch(x,y,z):\n",
    "  #Acceleration around X\n",
    "  acc_x_accl=[]\n",
    "\n",
    "  #Acceleration around Y\n",
    "  acc_y_accl=[]\n",
    "\n",
    "  #Acceleration arounf Z\n",
    "  acc_z_accl=[]\n",
    "\n",
    "\n",
    "  for (x_mean,y_mean,z_mean) in zip(x,y,z):\n",
    "    acc_x_accl.append(float(\"{:.2f}\".format(x_mean*3.9)))\n",
    "    acc_y_accl.append(float(\"{:.2f}\".format(y_mean*3.9)))\n",
    "    acc_z_accl.append(float(\"{:.2f}\".format(z_mean*3.9)))\n",
    "\n",
    "\n",
    "  acc_pitch=[]\n",
    "  acc_roll=[]\n",
    "  acc_yaw=[]\n",
    "\n",
    "  for (acc_x,acc_y,acc_z) in zip(acc_x_accl,acc_y_accl,acc_z_accl):\n",
    "    if acc_y==0 and acc_z==0:\n",
    "      value_pitch=-0.1\n",
    "    else:\n",
    "      value_pitch=180 * math.atan (acc_x/math.sqrt(acc_y*acc_y + acc_z*acc_z))/M_PI\n",
    "    if acc_x ==0 and acc_z==0:\n",
    "      value_roll=-0.1\n",
    "      value_yaw=-0.1\n",
    "    else:\n",
    "      value_roll = 180 * math.atan (acc_y/math.sqrt(acc_x*acc_x + acc_z*acc_z))/M_PI\n",
    "      value_yaw = 180 * math.atan (acc_z/math.sqrt(acc_x*acc_x + acc_z*acc_z))/M_PI\n",
    "    value_pitch=float(\"{:.2f}\".format(value_pitch))\n",
    "    value_roll=float(\"{:.2f}\".format(value_roll))\n",
    "    value_yaw=float(\"{:.2f}\".format(value_yaw))\n",
    "    acc_pitch.append(value_pitch)\n",
    "    acc_roll.append(value_roll)\n",
    "    acc_yaw.append(value_yaw)\n",
    "  return acc_pitch,acc_roll,acc_yaw\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "#Sliding Window to null values\n",
    "def fill_null(data):\n",
    "  for col in data.columns:\n",
    "    null_indexes=data[data[col].isnull()].index.tolist()\n",
    "    #print(\"For \",col)\n",
    "    for ind in null_indexes:\n",
    "      #print(\" Got null value at \",ind)\n",
    "      values=data.loc[ind-6:ind-1,col]\n",
    "      #print(\" Last 5 values \",values)\n",
    "      mean_val=values.mean()\n",
    "      data.loc[ind,col]=mean_val\n",
    "  return data\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# loading the data\n",
    "\n",
    "def load_data():\n",
    "    user_one = \"/Users/carbon/Desktop/Federated Learning/extrasensory_dataset/00EABED2-271D-49D8-B599-1D4A09240601.features_labels.csv.gz\"\n",
    "    # user_one=\"C:\\\\pdata\\\\extrasensory_dataset\\\\00EABED2-271D-49D8-B599-1D4A09240601.features_labels.csv.gz\"\n",
    "    df_one=pd.read_csv(user_one)\n",
    "    #accelerometer\n",
    "    df_acc=df_one.iloc[:,1:27]\n",
    "    df_acc=fill_null(df_acc)\n",
    "    #gyroscope\n",
    "    df_gyro=df_one.iloc[:,27:53]\n",
    "    df_gyro=fill_null(df_gyro)\n",
    "    #magnometer\n",
    "    df_magnet=df_one.iloc[:,53:84]\n",
    "    df_magnet=fill_null(df_magnet)\n",
    "    # watch accelerometer\n",
    "    #df_watch_acc=df_one.iloc[:,84:130]\n",
    "    # location\n",
    "    #df_location=df_one.iloc[:,139:156]\n",
    "\n",
    "    # For accelerometer\n",
    "    #mean values\n",
    "    acc_mean_x=df_acc['raw_acc:3d:mean_x']\n",
    "    acc_mean_y=df_acc['raw_acc:3d:mean_y']\n",
    "    acc_mean_z=df_acc['raw_acc:3d:mean_z']\n",
    "\n",
    "    acc_mean_x=acc_mean_x.replace({0:0.001})\n",
    "\n",
    "    #standard deviations\n",
    "    #acc_std_x=df_acc['raw_acc:3d:std_x']\n",
    "    #acc_std_y=df_acc['raw_acc:3d:std_y']\n",
    "    #acc_std_z=df_acc['raw_acc:3d:std_z']\n",
    "\n",
    "    (pitch,roll,yaw)=compute_roll_yaw_pitch(acc_mean_x,acc_mean_y,acc_mean_z)\n",
    "    df_one['acc_pitch']=pitch\n",
    "    df_one['acc_roll']=roll\n",
    "    df_one['acc_yaw']=yaw\n",
    "\n",
    "    #for gyroscope\n",
    "    gyro_mean_x=df_gyro['proc_gyro:3d:mean_x']\n",
    "    gyro_mean_y=df_gyro['proc_gyro:3d:mean_y']\n",
    "    gyro_mean_z=df_gyro['proc_gyro:3d:mean_z']\n",
    "\n",
    "    (pitch,roll,yaw)=compute_roll_yaw_pitch(gyro_mean_x,gyro_mean_y,gyro_mean_z)\n",
    "\n",
    "    df_one['gyro_pitch']=pitch\n",
    "    df_one['gyro_roll']=roll\n",
    "    df_one['gyro_yaw']=yaw\n",
    "\n",
    "    # For magnetometer\n",
    "    magno_mean_x=df_magnet['raw_magnet:3d:mean_x']\n",
    "    magno_mean_y=df_magnet['raw_magnet:3d:mean_y']\n",
    "    magno_mean_z=df_magnet['raw_magnet:3d:mean_z']\n",
    "\n",
    "    (pitch,roll,yaw)=compute_roll_yaw_pitch(magno_mean_x,magno_mean_y,magno_mean_z)\n",
    "\n",
    "    df_one['magno_pitch']=pitch\n",
    "    df_one['magno_roll']=roll\n",
    "    df_one['magno_yaw']=yaw\n",
    "\n",
    "    y=df_one[['label:FIX_running','label:FIX_walking','label:SITTING','label:SLEEPING','label:OR_standing']]\n",
    "\n",
    "    # to avoid null values\n",
    "    y['label:FIX_running']=y['label:FIX_running'].fillna(0)\n",
    "    y['label:FIX_walking']=y['label:FIX_walking'].fillna(0)\n",
    "    y['label:SITTING']=y['label:SITTING'].fillna(0)\n",
    "    y['label:SLEEPING']=y['label:SLEEPING'].fillna(0)\n",
    "    y['label:OR_standing']=y['label:OR_standing'].fillna(0)\n",
    "\n",
    "    #Check rows where all the recorded activities are zero ~ No activity recorded rows\n",
    "    list_of_indexs=[]\n",
    "    for i in range(len(y)):\n",
    "        run=y.iloc[i,0]\n",
    "        walk=y.iloc[i,1]\n",
    "        sit=y.iloc[i,2]\n",
    "        sleep=y.iloc[i,3]\n",
    "        stand=y.iloc[i,4]\n",
    "        activities=[run,walk,sit,sleep,stand]\n",
    "        count_ones=activities.count(1)\n",
    "        if walk==0 and run==0 and sit==0 and sleep==0 and stand==0:\n",
    "            list_of_indexs.append(i)\n",
    "        #check if more then 1 exists for different activities\n",
    "        elif count_ones>1:\n",
    "            list_of_indexs.append(i)\n",
    "\n",
    "    y=y.drop(list_of_indexs)\n",
    "    X=df_one.iloc[:,-9:]\n",
    "    X=X.drop(list_of_indexs)\n",
    "\n",
    "\n",
    "    return X,y\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# loading the data\n",
    "X,y=load_data()\n",
    "X = X.to_numpy()\n",
    "labels = y.to_numpy()\n",
    "\n",
    "# -----------------------------\n",
    "\n",
    "# # Creating a small balanced dataset\n",
    "\n",
    "X_1 = X[X['y'] ==  1].sample(n=150, random_state=42)\n",
    "X_2 = X[X['y'] ==  2].sample(n=150, random_state=42)\n",
    "X_3 = X[X['y'] ==  3].sample(n=150, random_state=42)\n",
    "X_4 = X[X['y'] ==  4].sample(n=150, random_state=42)\n",
    "\n",
    "small_X = pd.concat([X_1, X_2, X_3, X_4])\n",
    "small_X = small_X.sample(frac=1).reset_index(drop=True)\n",
    "small_y = small_X['y']\n",
    "small_X = small_X.drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate dataset\n",
    "# N_training = 100\n",
    "# N_validation = 50\n",
    "# N = N_training + N_validation\n",
    "\n",
    "\n",
    "# X0 = np.array([-1, 0]).T + 0.5 * np.random.normal(size=(N, 2))\n",
    "# X1 = np.array([1, 0]).T +  0.5 * np.random.normal(size=(N, 2))\n",
    "# X2 = np.array([0, 1]).T + 0.3 * np.random.normal(size=(N, 2))\n",
    "# X = np.concatenate([X0, X1, X2])\n",
    "\n",
    "# labels = np.zeros((3*N, 1))\n",
    "# labels[N:2*N] = 1\n",
    "# labels[2*N:] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "N_training = int(0.8 * N)\n",
    "N_validation = N - N_training\n",
    "\n",
    "# Training dataset\n",
    "indices = np.random.permutation(3 * N)\n",
    "trn_indices, val_indices = indices[:N_training], indices[N_training:]\n",
    "Tdataset = ([torch.tensor(X[trn_indices], dtype=torch.float32), torch.tensor(labels[trn_indices], dtype=torch.float32)])\n",
    "Vdataset = ([torch.tensor(X[val_indices], dtype=torch.float32), torch.tensor(labels[val_indices], dtype=torch.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1939, 9)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an RBF layer where the dimensionality of the input feature is 2,\n",
    "# the number of kernels is 3, and 2 output features\n",
    "\n",
    "# euclidean norm\n",
    "def euclidean_norm(x):\n",
    "    return torch.norm(x, p=2, dim=-1)\n",
    "\n",
    "\n",
    "# Gaussian RBF\n",
    "def rbf_gaussian(x):\n",
    "    return (-x.pow(2)).exp()\n",
    "\n",
    "rbf = RBFLayer(in_features_dim=9,\n",
    "               num_kernels=4,\n",
    "               out_features_dim=4,\n",
    "               radial_function=rbf_gaussian,\n",
    "               norm_function=euclidean_norm,\n",
    "               normalization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (32) to match target batch_size (160).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m optimiser\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m y \u001b[39m=\u001b[39m rbf(x)\n\u001b[0;32m---> 24\u001b[0m loss \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mCrossEntropyLoss()(y, labels\u001b[39m.\u001b[39;49mlong())\n\u001b[1;32m     25\u001b[0m epoch_trn_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     26\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (160)."
     ]
    }
   ],
   "source": [
    "optimiser = torch.optim.Adam(rbf.parameters(), lr=1e-3)\n",
    "epoch = 0\n",
    "batch_size = 32\n",
    "trn_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(512):\n",
    "    indices = np.random.permutation(N_training)\n",
    "    batch_idx = 0\n",
    "    epoch_trn_losses = []\n",
    "    epoch_val_losses = []\n",
    "    epoch_trn_accs = []\n",
    "    epoch_val_accs = []\n",
    "    \n",
    "    # Epoch training\n",
    "    while batch_idx < N_training:\n",
    "        idxs = indices[batch_idx:batch_idx + batch_size]\n",
    "        x = Tdataset[0][idxs]\n",
    "        labels = Tdataset[1][idxs].flatten()\n",
    "        \n",
    "        # Compute loss\n",
    "        optimiser.zero_grad()\n",
    "        y = rbf(x)\n",
    "        loss = nn.CrossEntropyLoss()(y, labels.long())\n",
    "        epoch_trn_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        batch_idx += batch_size\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Compute validation\n",
    "            x = Vdataset[0]\n",
    "            labels = Vdataset[1].flatten()\n",
    "            y = rbf(x)\n",
    "            loss = nn.CrossEntropyLoss()(y, labels.long())\n",
    "            epoch_val_losses.append(loss.item())\n",
    "            \n",
    "    trn_losses.append(np.mean(epoch_trn_losses))\n",
    "    val_losses.append(np.mean(epoch_val_losses))\n",
    "\n",
    "plt.plot(trn_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Training plot')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
